
Training took 1:13 - Final test loss 0.0557
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:9 - Final test loss 0.0537
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:10 - Final test loss 0.0544
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:10 - Final test loss 0.0532
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:10 - Final test loss 0.0507
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:10 - Final test loss 0.0537
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:11 - Final test loss 0.0536
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:13 - Final test loss 0.0547
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:12 - Final test loss 0.0550
 Parameters : 
	 - layers : 2 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:39 - Final test loss 0.0572
 Parameters : 
	 - layers : 4 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 1:56 - Final test loss 0.0593
 Parameters : 
	 - layers : 4 
	 --batch_size : 256
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : sgd  
Training took 2:9 - Final test loss 0.0292
 Parameters : 
	 - layers : 4 
	 --batch_size : 256
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : adam  
Training took 2:9 - Final test loss 0.0286
 Parameters : 
	 - layers : 4 
	 --batch_size : 256
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : adam  
Training took 2:8 - Final test loss 0.0571
 Parameters : 
	 - layers : 4 
	 --batch_size : 256
	 -epochs :20 
	 -learning_rate : 0.01  
	 -Optimizer : adam  
Training took 2:5 - Final test loss 0.0325
 Parameters : 
	 - layers : 4 
	 --batch_size : 256
	 -epochs :20 
	 -learning_rate : 0.002  
	 -Optimizer : adam  
Training took 2:10 - Final test loss 0.0286
 Parameters : 
	 - layers : 4 
	 --batch_size : 256
	 -epochs :20 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 1:38 - Final test loss 0.0322
 Parameters : 
	 - layers : 4 
	 --batch_size : 64
	 -epochs :20 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 1:45 - Final test loss 0.0400
 Parameters : 
	 - layers : 4 
	 --batch_size : 32
	 -epochs :20 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 1:44 - Final test loss 0.0299
 Parameters : 
	 - layers : 4 
	 --batch_size : 32
	 -epochs :20 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 2:42 - Final test loss 0.0368
 Parameters : 
	 - layers : 6 
	 --batch_size : 32
	 -epochs :20 
	 -learning_rate : 0.00125  
	 -Optimizer : adam  
Training took 11:45 - Final test loss 0.0371
 Parameters : 
	 - layers : 6 
	 --batch_size : 128
	 -epochs :100 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 23:26 - Final test loss 0.0349
 Parameters : 
	 - layers : 6 
	 --batch_size : 256
	 -epochs :200 
	 -learning_rate : 0.00125  
	 -Optimizer : adam  
Training took 4:16 - Final test loss 0.0016
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,
        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]) 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 4:19 - Final test loss 0.0017
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,
        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]) 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 4:27 - Final test loss 0.0018
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :50 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:15 - Final test loss 0.0066
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :3 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:5 - Final test loss 0.0154
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :1 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:5 - Final test loss 0.0156
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :1 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:5 - Final test loss 0.0132
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :1 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:5 - Final test loss 0.0149
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :1 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:5 - Final test loss 0.0139
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :1 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:5 - Final test loss 0.0149
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :1 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:5 - Final test loss 0.0147
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :1 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 4:23 - Final test loss 0.0017
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :50 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 0:10 - Final test loss 0.0092
 Parameters : 
	 - layers : 2 
	 --batch_size : 256
	 -epochs :2 
	 -learning_rate : 0.0015  
	 -Optimizer : adam  
Training took 1:16 - Final test loss 0.0020
 Parameters : 
	 - layers : 2 
	 --batch_size : 32
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : adam  
Training took 1:16 - Final test loss 0.0016
 Parameters : 
	 - layers : 2 
	 --batch_size : 32
	 -epochs :20 
	 -learning_rate : 0.001  
	 -Optimizer : adam  